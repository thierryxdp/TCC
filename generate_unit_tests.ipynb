{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "#!pip install --upgrade crosshair-tool\n",
    "import inspect\n",
    "import openai\n",
    "import re\n",
    "import importlib\n",
    "import ast\n",
    "from IPython.display import Markdown, display\n",
    "from utils import *\n",
    "from utils_cfg import GeradorDeRequisitos, createControlFlowGraph\n",
    "from problems import getProblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processo de Geração de Testes Unitarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    prompt_tokens_cost = 0.0005\n",
    "    completion_tokens_cost = 0.0015\n",
    "    total_cost = 0\n",
    "    openai.api_key = \"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\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    start_time = time.time()\n",
    "    response = None\n",
    "    while time.time() - start_time < 90:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        if 'choices' in response and len(response['choices']) > 0:\n",
    "            total_cost += prompt_tokens_cost * response['usage']['prompt_tokens']/1000\n",
    "            total_cost += completion_tokens_cost * response['usage']['completion_tokens']/1000\n",
    "            return response['choices'][0]['message']['content'], round(total_cost, 8)\n",
    "\n",
    "    return \"Request timed out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_text(theme, context, final_tests, function_lines):\n",
    "    template_text = \"\"\n",
    "    template_text += \"Vou te passar testes em pytest para um problema de programação em python sobre o tema (\"\n",
    "    template_text += theme\n",
    "    template_text += \").\\nEm seguida te passarei o contexto do problema e a solução. Quero que você use os meus testes, aqueles que forneci, mas altere os parâmetros para valores que fazem sentido no problema dado, apenas valores que aparecem no problema real.\\n É muito importante que cada teste tenha somente um assert, se tiver mais de um assert é necessario botar em outro teste com nome do teste diferente.\\n\\n\"\n",
    "    template_text += \"Meus Testes:\\n\\n\"\n",
    "    for test in final_tests:\n",
    "        template_text += test\n",
    "        template_text += \"\\n\"\n",
    "    template_text += \"Contexto do Problema:\\n\"\n",
    "    template_text += context + \"\\n\\nSolução do problema em python:\\n\"\n",
    "    for line in function_lines:\n",
    "        template_text += line + \"\\n\"\n",
    "\n",
    "    template_text += \"\\n\\nÉ MUITO IMPORTANTE que a quantidade de testes se mantenha. Só é válido adicionar mais testes se eles percorrem caminhos de execução diferentes!! Por fim, ponha os testes neste formato:\\n\"\n",
    "    template_text += \"def test_1():\\n   assert...\\n\\n\"\n",
    "    template_text += \"def test_2():\\n   assert...\\n\\n\"\n",
    "    template_text += \"def test_3():\\n   assert...\\n\\n\"\n",
    "    template_text += \"Esse formato consiste em apenas um assert para cada teste e duas linhas para cada teste: a linha de definicao dele e em seguida a linha de assert. Mesmo que a linha de assert fique grande, precisa ser tudo em apenas uma linha.\"\n",
    "    \n",
    "    return template_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_tests_from_response(gpt_response):\n",
    "    gpt_tests = []\n",
    "    current_test = \"\"\n",
    "    test_start = False\n",
    "    gpt_response += \"\\n\"\n",
    "    lines = gpt_response.split('\\n')\n",
    "    for line in lines:\n",
    "        if \"def\" in line and \"test\" in line:\n",
    "            test_start = True\n",
    "        if (len(line) == 0 and len(current_test) > 0):\n",
    "            gpt_tests.append(current_test)\n",
    "            current_test = \"\"\n",
    "            test_start = False\n",
    "        if (test_start):\n",
    "            current_test += line + \"\\n\"\n",
    "    \n",
    "    return gpt_tests[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_test(test_string, module_name):\n",
    "    parts = test_string.split(\" == \")\n",
    "    if len(parts) != 2:\n",
    "        return \"Invalid test format.\"\n",
    "    function_call, expected_result = parts\n",
    "    if \"(\" in function_call and \")\" in function_call:\n",
    "        function_name, arg_str = function_call.split(\"(\", 1)\n",
    "        args = eval(\"[\" + arg_str[:-1] + \"]\")\n",
    "    else:\n",
    "        return \"Invalid function call format.\"\n",
    "\n",
    "    try:\n",
    "        module = importlib.import_module(module_name)\n",
    "        function = getattr(module, function_name)\n",
    "        result = function(*args)\n",
    "\n",
    "        if result == eval(expected_result):\n",
    "            return f\"Test passed: {test_string}\"\n",
    "        else:\n",
    "            return f\"Test failed: {test_string}\"\n",
    "    except ImportError:\n",
    "        return f\"Module '{module_name}' not found.\"\n",
    "    except AttributeError:\n",
    "        return f\"Function '{function_name}' not found in module '{module_name}'.\"\n",
    "    except ValueError:\n",
    "        return \"Generated values are not valid for the solution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_commas(string: str) -> str:\n",
    "    return string.replace('\"', '\\\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_tests(gpt_tests, function_lines):\n",
    "    file_name = getMethodName(function_lines) + \".py\"\n",
    "    loop_blank_spaces, loop_commands = [], []\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"import re\" + \"\\n\")\n",
    "        file.write(\"import inspect\" + \"\\n\")\n",
    "        file.write(\"from typing import List, Dict\\n\")\n",
    "        file.write(function_lines[0] + \"\\n\")\n",
    "        file.write(\" \" * 4 + \"with open(\\\"holder.txt\\\", \\\"a\\\", encoding='utf-8') as file:\" + \"\\n\")\n",
    "        file.write(\" \" * 8 + \"file.write(\\\"\" + function_lines[0].strip().replace(\"def \", \"enter: \").replace(\"):\", \")\") + \"\\\" + \\\"\\\\n\\\")\" + \"\\n\")\n",
    "        for i in range(1, len(function_lines)):\n",
    "            if (count_blank_spaces(function_lines[i]) in loop_blank_spaces):\n",
    "                index = loop_blank_spaces.index(count_blank_spaces(function_lines[i]))\n",
    "                file.write(\" \" * 4 + \" \" * count_blank_spaces(loop_commands[index]) + \"file.write(\\\"\" + replace_commas(loop_commands[index].strip()) + \"\\\" + \\\"\\\\n\\\")\" + \"\\n\") \n",
    "            if not \"else\" in function_lines[i] and not \"elif\" in function_lines[i]:\n",
    "                file.write(\" \" * 4 + \" \" * count_blank_spaces(function_lines[i]) + \"file.write(\\\"\" + replace_commas(function_lines[i].strip()) + \"\\\" + \\\"\\\\n\\\")\" + \"\\n\")\n",
    "            if \"return\" in function_lines[i]:\n",
    "                file.write(\" \" * 4 + \" \" * count_blank_spaces(function_lines[i]) + \"file.write(\\\"\" + replace_commas(function_lines[0].strip().replace(\"def \", \"exit: \").replace(\"):\", \")\")) + \"\\\" + \\\"\\\\n\\\")\" + \"\\n\")\n",
    "            file.write(\" \" * 4 + function_lines[i] + \"\\n\")\n",
    "            if \"for\" in function_lines[i] or \"while\" in function_lines[i]:\n",
    "                file.write(\" \" * 8 + \" \" * count_blank_spaces(function_lines[i]) + \"file.write(\\\"\" + replace_commas(function_lines[i].strip()) + \"\\\" + \\\"\\\\n\\\")\" + \"\\n\")\n",
    "                loop_blank_spaces.append(count_blank_spaces(function_lines[i]))\n",
    "                loop_commands.append(function_lines[i])\n",
    "            if \"else\" in function_lines[i] or \"elif\" in function_lines[i]:\n",
    "                file.write(\" \" * 8 + \" \" * count_blank_spaces(function_lines[i]) + \"file.write(\\\"\" + replace_commas(function_lines[i].strip()) + \"\\\" + \\\"\\\\n\\\")\" + \"\\n\")\n",
    "\n",
    "    module = importlib.import_module(file_name.split(\".\")[0])\n",
    "    getattr(module, getMethodName(function_lines), None)\n",
    "    valid_tests = []\n",
    "    \n",
    "    for test in gpt_tests:\n",
    "        pattern = r'assert (.*)'\n",
    "        match = re.search(pattern, test)\n",
    "        if match:\n",
    "            matched_content = match.group(1)\n",
    "            execution_result = execute_test(matched_content, file_name.split(\".\")[0])\n",
    "            if (\"Test passed\" in execution_result):\n",
    "                with open(\"holder.txt\", \"a\", encoding='utf-8') as file:\n",
    "                    file.write(\"test_delimiter\\n\")\n",
    "                valid_tests.append(test)\n",
    "            else:\n",
    "                filename = \"holder.txt\"\n",
    "                with open(filename, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                \n",
    "                index = None\n",
    "                for i, line in enumerate(lines):\n",
    "                    if \"test_delimiter\" in line:\n",
    "                        index = i\n",
    "\n",
    "                if index is not None:\n",
    "                    with open(filename, \"w\") as file:\n",
    "                        file.writelines(lines[:index+1])\n",
    "\n",
    "    result, current_list = [], []\n",
    "    with open(\"holder.txt\", \"r\", encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == \"test_delimiter\":\n",
    "                if current_list:\n",
    "                    result.append(current_list)\n",
    "                current_list = []\n",
    "            else:\n",
    "                current_list.append(line)\n",
    "\n",
    "    if current_list:\n",
    "        result.append(current_list)\n",
    "\n",
    "    remove_file(file_name)\n",
    "    remove_all_files(\"__pycache__\")\n",
    "    remove_file(\"holder.txt\")\n",
    "\n",
    "    return result, valid_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crosshair_tests(function_lines):\n",
    "    file_name = \"solution.py\"\n",
    "\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"import inspect\\n\")\n",
    "        file.write(\"import re\\n\")\n",
    "        file.write(\"from typing import List, Dict\\n\")\n",
    "        for line in function_lines:\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "    #tests = !crosshair cover --example_output_format=pytest --coverage_type=path solution.{getMethodName(function_lines)} --max_uninteresting_iterations=100\n",
    "    tests = !crosshair cover --example_output_format=pytest --coverage_type=path solution.{getMethodName(function_lines)} --per_condition_timeout=100\n",
    "    remove_file(file_name)\n",
    "    \n",
    "    start_index = 0\n",
    "    for i in range(0, len(tests)):\n",
    "        if ('def' in tests[i]):\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    realtests = []\n",
    "    current_test = tests[start_index]\n",
    "    for i in range(start_index + 1, len(tests)):\n",
    "        if 'test_' in tests[i]:\n",
    "            realtests.append(current_test)\n",
    "            current_test = tests[i]\n",
    "        else:\n",
    "            current_test += \"\\n\" + tests[i] \n",
    "\n",
    "    final_tests = []\n",
    "    for rt in realtests:\n",
    "        if 'pytest.raises' not in rt and \"None\" not in rt:\n",
    "            final_tests.append(rt)\n",
    "\n",
    "    return final_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimal_tests(nodes, test_nodes):\n",
    "    covered_nodes = set()\n",
    "    minimal_tests = []\n",
    "    sorted_test_nodes = sorted(test_nodes, key=len, reverse=True)\n",
    "\n",
    "    for i in range(len(sorted_test_nodes)):\n",
    "        uncovered_nodes = [node for node in sorted_test_nodes[i] if node not in covered_nodes]\n",
    "        \n",
    "        if len(uncovered_nodes) > 0:\n",
    "            minimal_tests.append(sorted_test_nodes[i])\n",
    "            covered_nodes.update(uncovered_nodes)\n",
    "\n",
    "        if len(covered_nodes) == len(nodes):\n",
    "            break\n",
    "\n",
    "    return minimal_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class CoverageCriteria(Enum):\n",
    "    NONE = 0\n",
    "    NODES = 1\n",
    "    EDGES = 2\n",
    "    PAIR_EDGES = 3\n",
    "\n",
    "def filter_tests(result, criteria, gpt_tests):\n",
    "    filename = \"problem_solution\"\n",
    "    nodes = GeradorDeRequisitos(\"./\"+filename+\".py\", filename)\n",
    "    all, tests = [], []\n",
    "\n",
    "    for test in result:\n",
    "        tests_n = []\n",
    "        for line in test:\n",
    "            for n in nodes:\n",
    "                line_cleaned = line.replace('\\n', '').replace(' ', '')\n",
    "                if line_cleaned and line_cleaned in n[2].replace('\\n', '').replace(' ', ''):\n",
    "                    tests_n.append(n[0])\n",
    "        tests.append(remove_adjacent_duplicates(tests_n))\n",
    "\n",
    "    if criteria == CoverageCriteria.NODES:\n",
    "        all = [sublist[0] for sublist in nodes]\n",
    "    elif criteria == CoverageCriteria.EDGES:\n",
    "        if len(nodes) <= 1:\n",
    "            all = [sublist[0] for sublist in nodes]\n",
    "        else:\n",
    "            for node in nodes:\n",
    "                    for idx, neighbour in enumerate(node[1]):\n",
    "                        all.append(tuple([node[0],node[1][idx]]))\n",
    "\n",
    "            test_edges, current_test_edges = [], []\n",
    "            for test in tests:\n",
    "                for i in range(len(test) - 1):\n",
    "                    current_test_edges.append(tuple([test[i], test[i+1]]))\n",
    "                test_edges.append(current_test_edges)\n",
    "                current_test_edges = []\n",
    "            tests = test_edges\n",
    "    elif criteria == CoverageCriteria.PAIR_EDGES:\n",
    "        if len(nodes) <= 1:\n",
    "            all = [sublist[0] for sublist in nodes]\n",
    "        else:\n",
    "            all_pairs, tests_pair_edges = [], []\n",
    "            with open(\"requisitos \" + filename + \".txt\", \"r\") as file:\n",
    "                for line in file:\n",
    "                    if \"par de arcos\" in line:\n",
    "                        all_pairs = ast.literal_eval(line[line.index(\":\") + 1:].strip())\n",
    "            for tn in tests:\n",
    "                pairs, current_pair, unique_pairs, test_pair_edge = [], [], [], []\n",
    "                for node in tn:\n",
    "                    current_pair.append(node)\n",
    "                    if (len(current_pair) == 3):\n",
    "                        pairs.append(current_pair)\n",
    "                        current_pair = current_pair[1:]\n",
    "                \n",
    "                if (len(current_pair) < 3 and len(pairs) == 0):\n",
    "                    pairs.append(current_pair)\n",
    "\n",
    "                for sublist in pairs:\n",
    "                    if sublist not in unique_pairs:\n",
    "                        unique_pairs.append(sublist)\n",
    "\n",
    "                for sublist in unique_pairs:\n",
    "                    test_pair_edge.append(tuple(sublist))\n",
    "\n",
    "                tests_pair_edges.append(test_pair_edge)\n",
    "\n",
    "            all_pairs_tuples = []\n",
    "            for sublist in all_pairs:\n",
    "                all_pairs_tuples.append(tuple(sublist))\n",
    "\n",
    "            all = all_pairs_tuples\n",
    "            tests = tests_pair_edges\n",
    "    else:\n",
    "        print(\"Coverage Criteria not found.\")\n",
    "\n",
    "    print(len(all), 'Requisitos a satisfazer: ', all, '\\n')\n",
    "    minimal_tests = find_minimal_tests(all, tests)\n",
    "    \n",
    "    filtered_list = []\n",
    "    satisfied_requirements = set()\n",
    "    for minimal_test in minimal_tests:\n",
    "        number_requirements_satisfied = len(satisfied_requirements)\n",
    "        filtered_test = gpt_tests[tests.index(minimal_test)]\n",
    "        line = \"Novos requisitos satisfeitos pelo teste: \"\n",
    "        for req in all:\n",
    "            if req in minimal_test and req not in satisfied_requirements:\n",
    "                satisfied_requirements.add(req)\n",
    "                line += f\"\\033[92m{req}\\033[0m \"  # Green for new requirement satisfied by this test\n",
    "            elif req in minimal_test:\n",
    "                line += f\"\\033[90m{req}\\033[0m \"  # Grey for previously satisfied requirement\n",
    "        line += f\"-> {len(satisfied_requirements) - number_requirements_satisfied}/{len(all)}\"\n",
    "        print(line)\n",
    "        print(filtered_test)\n",
    "        filtered_list.append(filtered_test)\n",
    "        \n",
    "    print(f\"Total de requisitos satisfeitos: \\033[92m{len(satisfied_requirements)}/{len(all)}\\033[0m. Percentual de cobertura: \\033[92m{round(len(satisfied_requirements)/len(all)*100,2)}%\\033[0m\")\n",
    "        \n",
    "    remove_file('requisitos problem_solution.txt')\n",
    "    \n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tests_results(crosshair_tests, crosshair_result, function_lines, criteria, context, theme):\n",
    "    display(Markdown(f\"<font color=magenta>Testes CrossHair Filtrados</font>\"))\n",
    "    filtered_crosshair_tests = filter_tests(crosshair_result, criteria, crosshair_tests)\n",
    "    display(Markdown(f\"<font color=magenta>Testes ChatGpt</font>\"))\n",
    "    template_text = get_template_text(theme, context, filtered_crosshair_tests, function_lines)\n",
    "    gpt_response, cost = get_completion(template_text)\n",
    "    gpt_tests = get_gpt_tests_from_response(gpt_response)\n",
    "    for t in gpt_tests:\n",
    "        print(t)\n",
    "    display(Markdown(f\"<font color=magenta>Testes ChatGpt Filtrados</font>\"))\n",
    "    gpt_result, valid_tests = validate_tests(gpt_tests, function_lines)\n",
    "    filter_tests(gpt_result, criteria, valid_tests)\n",
    "    \n",
    "    display(Markdown(f\"<font color=magenta>Resultado Final</font>\"))\n",
    "    filter_tests(crosshair_result + gpt_result, criteria, crosshair_tests + valid_tests)    \n",
    "    print(f\"Custo da Requisição à api do ChatGpt: \\033[92m${cost}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_re_usage(function_lines):\n",
    "    new_function_lines = []\n",
    "    i = 0\n",
    "    while i < len(function_lines):\n",
    "        if 're.fullmatch' in function_lines[i]:\n",
    "            i += 2\n",
    "        else:\n",
    "            new_function_lines.append(function_lines[i])\n",
    "            i += 1\n",
    "        \n",
    "    return new_function_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unit_tests(function_lines, context, theme, criteria=CoverageCriteria.NONE, debugger=False):\n",
    "    elapsed_crosshair_time, elapsed_nodes_time, elapsed_edges_time, elapsed_pairedges_time = 0, 0, 0, 0\n",
    "    start_time = time.time()\n",
    "    crosshair_tests = get_crosshair_tests(function_lines)\n",
    "    function_lines = remove_re_usage(function_lines)\n",
    "    result, valid_tests = validate_tests(crosshair_tests, function_lines)\n",
    "    end_time = time.time()\n",
    "    elapsed_crosshair_time = round(end_time - start_time, 2)\n",
    "    with open(\"problem_solution.py\", 'w') as file:\n",
    "        for line in function_lines:\n",
    "            file.write(line + \"\\n\")\n",
    "            \n",
    "    display(Markdown(f\"<font color=yellow>Grafo de fluxo de controle do problema:</font>\"))\n",
    "    createControlFlowGraph()\n",
    "    \n",
    "    display(Markdown(f\"<font color=magenta>Testes CrossHair</font>\"))\n",
    "    \n",
    "    print(valid_tests[0])\n",
    "    print(valid_tests[1])\n",
    "    print(valid_tests[2])\n",
    "    \n",
    "    print(\"...\")\n",
    "    \n",
    "    print(valid_tests[len(valid_tests)-3])\n",
    "    print(valid_tests[len(valid_tests)-2])\n",
    "    print(valid_tests[len(valid_tests)-1])\n",
    "    \n",
    "    if criteria == CoverageCriteria.NONE or criteria == CoverageCriteria.NODES:\n",
    "        display(Markdown(f\"<font color=yellow>Testes Critério de Nós</font>\"))\n",
    "        start_time = time.time()\n",
    "        get_tests_results(valid_tests, result, function_lines, CoverageCriteria.NODES, context, theme)\n",
    "        end_time = time.time()\n",
    "        elapsed_nodes_time = round(end_time - start_time, 2)\n",
    "    \n",
    "    if criteria == CoverageCriteria.NONE or criteria == CoverageCriteria.EDGES:\n",
    "        display(Markdown(f\"<font color=yellow>Testes Critério de Arestas</font>\"))\n",
    "        start_time = time.time()\n",
    "        get_tests_results(valid_tests, result, function_lines, CoverageCriteria.EDGES, context, theme)\n",
    "        end_time = time.time()\n",
    "        elapsed_edges_time = round(end_time - start_time, 2)\n",
    "\n",
    "    if criteria == CoverageCriteria.NONE or criteria == CoverageCriteria.PAIR_EDGES:\n",
    "        display(Markdown(f\"<font color=yellow>Testes Critério de Par de Arestas</font>\"))\n",
    "        start_time = time.time()\n",
    "        get_tests_results(valid_tests, result, function_lines, CoverageCriteria.PAIR_EDGES, context, theme)\n",
    "        end_time = time.time()\n",
    "        elapsed_pairedges_time = round(end_time - start_time, 2)\n",
    "    \n",
    "    remove_file(\"problem_solution.py\")\n",
    "    if elapsed_crosshair_time > 0:\n",
    "        print(f\"Tempo de Execução do CrossHair: \\033[92m{elapsed_crosshair_time}s\\033[0m\")\n",
    "    if elapsed_nodes_time > 0:\n",
    "        print(f\"Tempo de Execução do ChatGpt para o critério de nós: \\033[92m{elapsed_nodes_time}s\\033[0m\")\n",
    "    if elapsed_edges_time > 0:\n",
    "        print(f\"Tempo de Execução do ChatGpt para o critério de arestas: \\033[92m{elapsed_edges_time}s\\033[0m\")\n",
    "    if elapsed_pairedges_time > 0:\n",
    "        print(f\"Tempo de Execução do ChatGpt para o critério de par arestas: \\033[92m{elapsed_pairedges_time}s\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [160], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m criteria \u001b[38;5;241m=\u001b[39m CoverageCriteria\u001b[38;5;241m.\u001b[39mPAIR_EDGES\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m problem \u001b[38;5;129;01min\u001b[39;00m problems:\n\u001b[1;32m----> 4\u001b[0m     generate_unit_tests(problem[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m), problem[\u001b[38;5;241m1\u001b[39m], problem[\u001b[38;5;241m2\u001b[39m], criteria, \u001b[43mtrue\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "problems = getProblems()\n",
    "criteria = CoverageCriteria.PAIR_EDGES\n",
    "for problem in problems:\n",
    "    generate_unit_tests(problem[0].split('\\n'), problem[1], problem[2], criteria, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def count(frase: str, letra: chr) -> int:\\n    contador = 0\\n    for l in frase.lower():\\n        if letra == l:\\n            contador += 1\\n    return contador\\n']\n"
     ]
    }
   ],
   "source": [
    "def count(frase: str, letra: chr) -> int:\n",
    "    contador = 0\n",
    "    for l in frase.lower():\n",
    "        if letra == l:\n",
    "            contador += 1\n",
    "    return contador\n",
    "\n",
    "source_code = inspect.getsource(count)\n",
    "print([source_code])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
